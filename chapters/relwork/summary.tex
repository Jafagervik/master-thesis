
In general, both autoencoders with linear, convolutional, or recurrent layers, as well as clustering algorithms and more traditional \acrshort{ml} methods have seen many use-cases within \acrshort{das} research. However, in later years with the later additions of both attention layers, or even \acrshort{gan}s \cite{goodfellow2014generative, goodfellow2016nips}, more novel approaches are being researched. By introducing channel attention and spatial attention to \acrshort{cnn}, one article \cite{eage:/content/journals/10.1111/1365-2478.13355} finds good results for denoising \acrshort{das} signals. 

One key aspect of \acrshort{ann}s is the necessity of larger train datasets, compared to clustering techniques. For more vision-based tasks, data augmentation techniques such as cropping, resizing or color grading can increase the available datasets. Another way to increase the total amount of train data is by leveraging \acrshort{gan}s. After training a \acrshort{gan} model, the generator can be used to produce data that is similar to already collected data. This has yielded great results on \acrshort{das} data \cite{Shiloh:19}, and can be a great way to provide more train-data, which in turn can help models detect anomalies more accurately, by being trained on a broader variety of data. \\


\section{Summary}

As we have seen in this chapter, anomaly detection has a long history with research and trial with many algorithms. From more well-known ML techniques such as Bayesian networks, knn and isolation forests to more novel deep learning algorithms utilizing LSTMs and \acrshort{gan} networks. \\

The development and findings of several of these algorithms bring us directly to this project. We continue the research on anomaly detection by trying to improve unsupervised learning algorithms utilizing more novel deep learning algorithms, not only as sole models to solve a problem but also as a pre-processor model for supervised learning algorithms to be used on multi-sensor das data. We do so by improving and continuing \texttt{Judas} (formerly known as Emerald.jl), a processing and analysis package. We also distinguish our result by opting for a more novel language of choice, Julia, to not only demonstrate small examples of data science or data processing but as a serious contender for the language of choice when implementing libraries or applications dealing with big data, distributed computing and artificial intelligence
Anomaly detection, sometimes referred to as outlier detection, is highly relevant within \acrshort{das} research. In 2017, several classical \acrshort{ml} techniques such as Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Naive Bayes (NB), and Restricted Boltzmann Machine (RBM) are being compared to discriminative models including \acrshort{ann}s \cite{app7080841}. Variations of isolation forests is shown to be able to perform fault detection for mining conveyors\cite{WIJAYA2022110330}. \\

As previously mentioned in chapter \ref{chap:introduction}, label-free anomaly detection has the advantage of requiring a lot less manual labour and can be adapted to multiple datasets. A model that require only normal-state data, utilizing both autoencoders in combination with the K-means clustering technique, have proven to yield great results, even beating supervised methods \cite{s23084094}. \\ 

a, \cite{10.14778/3538598.3538602} \cite{10.1145/3444690}.

These papers show how anomaly detection on \acrshort{das} data is highly relevant, and how . However, they do not necessarily concern themselves with available computational power, overall memory consumption, or how to optimize these algorithms for real-time environments, where not only accuracy and fault tolerance, but also inference speed is of utmost importance.