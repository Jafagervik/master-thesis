\section{Programming Languages and Frameworks}

Our initial decision was to continue using Julia for our \acrshort{dl} methods, and train our models on the same dataset provided by \acrshort{cgf}. We created a Julia package called JudasNET, containing code for training different \acrshort{ai} models. However, due to severe computational limitations, we were unable to continue our work on the previous decision. With only 11GB of VRAM, the single \acrshort{gpu} available quickly became a bottleneck. Not only do we need to store batches of \acrshort{das} data matrices, but we also need to store the weights and biases of our models. By switching to a open source dataset, we would be able to use \Gls{idun}, and leverage multiple \acrshort{gpu}s and in general more computational power. Since the BANENOR dataset is close sourced, we would not be able to train our models on any other resources outside of those provided by \acrshort{cgf}.

Not only were our attempts at training models unsuccessful, Julia's main framework for \acrshort{ml} training \texttt{Flux.jl} does not provide built in tools for multigpu training. \\  

The more obvious choice would now be to use the Python package \texttt{Pytorch}, which is well established, documented and supports not only data parallel training \footnote{  \href{https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html}{https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html}}, but also distributed data parallel training \footnote{\href{https://pytorch.org/tutorials/intermediate/ddp_tutorial.html}{https://pytorch.org/tutorials/intermediate/ddp\_tutorial.html}}. What needs to be mentrioned is that Pytorch is heavily optimized for CUDA and NVIDIA \acrshort{gpu}s, and in general performs significantly better on these accelerators, compared to other alternatives such as AMD, NV, METAL and so on. \\ 

We want to create models where we don't need to change much of our code to run on different accelerators. We also want \acrshort{cgf} to  quickly be able to both continue training and running their own models. NVIDIA \acrshort{gpu}s are generally expensive, many of them reaching prices of tens of thousands of dollars per \acrshort{gpu}. We realize that this would not be feasible for \acrshort{cgf} to invest in right now, and thus we decided to look for other frameworks which supports a broader range of hardware accelerators. \\ 

Due to these constraints, we believe Tinygrad \cite{tinygrad} will be a well suited framework for our usecase.

\subsection{TinyGrad}











\mycomment{



DOI \cite{doi:10.1137/141000671}

\subsection{Flux.jl}
\label{back:flux}

Flux is a machine learning library written entirely in Julia released and published in 2018 \cite{Flux.jl-2018, Innes2018}. It allows the user to write their own machine-learning libraries. \acrshort{gpu} support is also native, through the inclusion of \texttt{CUDA.jl} \cite{Besard_2019}. We will be writing all of our models using Flux, or \texttt{Zygote.jl}, which \texttt{Flux.jl} is based on. 

Although \texttt{Flux.jl} is the preferred way to work with \acrshort{ai} in Julia, other prominent alternatives exists as well. \texttt{MLJ.jl} \cite{blaom2020flexible} \cite{Blaom2020} is a framework provided by the Alan Turing Institute\texttrademark, providing interfaces and functions for working with about 200 machine learning models. 

\texttt{Tensorflow.jl} is a Julia package which wraps Tensorflow functions from Python to be able to work with ethem

Julia has support for working on outlier detection as follows: \cite{muhr2022outlierdetectionjl}.

A list of all packages used in addition to Flux.jl can be found in the appendix \ref{app:packages}.

}