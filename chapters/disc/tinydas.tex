\section{TinyDAS}
\label{disc:tinydas}

As stated in Section \ref{meth:tinyoverview}, we designed TinyDAS around a set of guiding philosophies. After using TinyDAS for training and anomaly detection, we evaluate its performance against these principles.

\textbf{Support for memory-efficient training techniques, specifically half-precision:}
By setting the \lstinline|half_prec| variable in the configuration file to \lstinline|True|, all weights, biases, and data are computed using Float16. This significantly reduces memory usage and potentially speeds up training. However, users should note that gradient clipping and loss scaling may be necessary to avoid vanishing or exploding gradient issues. Furthermore, certain losses may need to be computed in single-precision to maintain accuracy.

\textbf{Scalability from single-device computation to multi-GPU systems:}
We are easily able to scale our training by enabling data parallelism as described in Section \ref{meth:dataloader} and shown in Code Listing \ref{code:dataloader}. Models can be efficiently copied to multiple devices, as seen in Code Listing \ref{code:main}, demonstrating TinyDAS's capability to utilize multi-GPU systems effectively.

\textbf{Hardware agnostic to ensure wide usability:}
The hardware agnosticism and clear separation of user-code and hardware-specific accelerator code is one of the main benefits inherited from Tinygrad \cite{tinygrad}. In our system, the \lstinline|get_gpus| function uses \lstinline|DEVICE.default| and a specified amount to choose the number of GPUs to be utilized for training, ensuring flexibility across different hardware configurations.

\textbf{Modular architecture, easily extendable with new models:}
By inheriting from the \lstinline|BaseAE| class, new models can easily be added to TinyDAS. This is demonstrated in the available code, where adding a model class and a configuration class is sufficient to train a new model. One limitation is the support for GAN models, which would require adjustments to both the trainer class and the \lstinline|BaseAE| model class to accommodate custom requirements. However, extending the model with other custom trainer classes remains feasible.

\textbf{Separation of core logic from data workflow for improved maintainability:}
As shown in the example usage in Listing \ref{code:tinymain}, a simple training workflow can be implemented in approximately 20 lines of code. This conciseness is achieved through the clear separation of core logic and data workflow, enhancing the system's maintainability and ease of use.

\textbf{Collection of different algorithms for anomaly detection:}
TinyDAS currently includes functionality for computing confusion matrices and finding relevant metrics for anomaly detection. This collection of algorithms provides users with various tools to perform effective anomaly detection tasks.

\textbf{Future potential for online anomaly detection in a live environment:}
TinyDAS can be adapted for online anomaly detection by utilizing the \lstinline|predict| and \lstinline|loss| methods provided by each model. By adding functionality for continuously reading from a datastream and finding anomalies in the desired timeframe, TinyDAS could be extended to support real-time anomaly detection in live environments.

\textbf{Model-agnostic approach to anomaly detection for broader applicability:}
The anomaly detection functions in TinyDAS are designed to work with any model that inherits from the base class. This model-agnostic approach allows users to apply all anomaly detection functionality without having to rewrite code for each specific model, as highlighted in Listing \ref{code:anomaly}. This design choice significantly enhances the system's flexibility and broader applicability across different types of models and anomaly detection scenarios.



\subsection{Sustainability within \acrshort{ai}}
\acrshort{ai} training is becoming an increasingly resource-intensive task on a yearly basis as stated by Google \cite{9499913}. TinyDAS incorporates two features that contribute to several of the \acrfull{un} \acrfull{sdg} \cite{UNSDGs}:
\begin{enumerate}
\item \textbf{Support for half-precision training}: Computing elements with 16 bits instead of 32 bits reduces overall training time and energy consumption.
\item \textbf{Early-stopping mechanism}: By stopping training when no significant improvements are observed, unnecessary resource usage is avoided.
\end{enumerate}
These features contribute to the following \acrshort{un} \acrshort{sdg} targets:
\begin{itemize}
\item Target 12.2: ''Achieve sustainable management and efficient use of natural resources.'' TinyDAS optimizes computational resource use, promoting sustainable management in AI development.
\item Target 9.4: ''Upgrade infrastructure and retrofit industries to make them sustainable, with increased resource-use efficiency.'' These features represent an upgrade in AI infrastructure, enhancing sustainability and efficiency.
\item Target 7.3: ''Double the global energy efficiency improvement rate.'' By reducing energy requirements for \acrshort{ai} training, TinyDAS contributes to overall energy efficiency in the tech sector.
\end{itemize}

The impact of these sustainability features extends beyond energy savings. They enable the development of more efficient AI models deployable on a wider range of devices, potentially democratizing AI technology, or in George Hotz' words: "Commoditizing the Petaflop" \cite{hotz2024commoditizing}. 