\section{BANENOR}
\label{disc:banenor}

\subsection{Experiment \rnum{1}: Data processing and resampling}

Analysis of the current processing pipeline reveals significant bottlenecks in the \texttt{load\_DAS\_files} function. Figure \ref{fig:judasextime} clearly demonstrates that loading the \acrshort{hdf5} files represents the bottleneck of of our approach. This largely stems from our fine-grained approach to parallelization within the \texttt{load\_DAS\_files} function. While we have implemented parallel processing techniques in specific parts of this function, the overwhelming majority of this function is strictly serial. The efficiency issues are further illustrated by Figure \ref{fig:judasefficiency}, showcasing a marked decline in processing efficiency across all time windows and durations.
Following the parallel loading phase, the function encounters another slowdown as it waits for the \texttt{ccds!} function to finish. This sequential processing step further hinders the overall performance limitations of our current implementation.
Despite these challenges, our approach of fine-grained parallelization within \texttt{load\_DAS\_files} and using memory mapping for signal data to binary files has provided valuable insights. These findings strongly indicate the necessity for a comprehensive overhaul of the current IO operations to fully leverage parallel techniques and reduce overall runtime.

Even if our approach to file loading proves ineffective, our method offers several advantages. Notably, it eliminates the need for repeated file loading, allowing users to load the required files for initial \acrshort{das} processing and then combine submatrices temporarily stored in binary files for further analyses. This approach reduces redundant data operations and increases overall flexibility. Furthermore, compared to similar programs at \acrshort{cgf}, our implementation substantially reduces overall RAM usage.

While our current implementation faces performance challenges, particularly in file loading and parallel efficiency, it has laid the groundwork for future optimizations. One potential improvement is to parallelize the entire \texttt{load\_DAS\_files} function, where each process $p$ only processes a subset of the filenames returned by the \texttt{find\_DAS\_file} function. This requires a parallel approach to the cumulative summation of submatrices, which the \texttt{ccds!} function currently does not support.


\subsection{Experiment \rnum{2}: Parallel Resampling}

\label{fig:resampling-benchmark}
The overall runtimes displayed in Figure \ref{fig:resampling-benchmark} show a significant improvement for all resampling rates and process counts compared to the serial execution. The runtime decreases by approximately a factor of 2 when moving from serial execution to our parallel version using 2 processes.

Figure \ref{fig:ex2heat} indicates that higher resampling rates benefit more from parallelization compared to lower ones. As expected, using only a single process with our parallel solution introduces minimal overhead, primarily from allocating the shared matrix to the parent process.

The sharp decline in efficiencies, shown in Figure \ref{fig:resampling_efficiency}, indicates strong diminishing returns as the number of processes increases. The most significant decline occurs between 4 and 8 processes, suggesting a potential bottleneck. Compared to the original \acrshort{das} signal matrix with 12500 channels, the overhead of spawning many processes for only 261 channels proves inefficient, particularly for lower resampling rates. This highlights that each process benefits from having a substantial workload, possibly due to Julia's function precompilation, as explained in Section \ref{meth:julia}. 

Our current implementation of parallel resampling does not utilize \acrshort{gpu}s. A potential improvement could be achieved by leveraging \texttt{CUDA.jl} to develop a GPU-accelerated resampling method that takes advantage of existent processing capabilities at \acrshort{cgf}, as shown in table \ref{tab:cgfsetup}. Efficient resampling algorithms for \acrshort{gpu}s have already been demonstrated, for instance, the work by Kim et al. (2013) \cite{kim2013efficient}.

Overall, this experiment demonstrates that our implementation of parallel channel decimation is best suited for more compute-intensive tasks, such as higher resampling rates. For this particular example, the optimal process count appears to be 4 processes, balancing improved performance with efficient resource utilization. Tests across multiple matrix sizes are needed to evaluate the amount of processes necessary further.