\section*{Reflections on Julia, Judas and TinyDAS}
\label{sec:juliaref}
After creating Judas, we were determined to continue implementing \acrshort{ai} models in Julia. Our initial work concluded in the JudasNET package\footnote{Available at \url{https://github.com/Jafagervik/JudasNET}}. We wanted to train our models on the same BANENOR dataset used in Section \ref{res:banenor}. Ultimately, there were two reasons for us turning away from Julia and opting for Python instead. 

The first reason is the limited amount of \acrshort{gpu}s available at \acrshort{cgf}; with only a single consumer grade level \acrshort{gpu} with \qty{11}{\si{\giga\byte}} \acrshort{vram} available, we would have to use smaller batches of data and maybe decrease window duration. This constraint led us to switch to the \gls{idun} cluster for training. Still, we were now not allowed to continue our work on the BANENOR dataset due to it being proprietary and classified data. \gls{idun} is a public server, and this data at \acrshort{cgf} is not allowed on public servers. This led us to work with the PubDAS \cite{spica2022pubdas, spica2023pubdas} dataset. 

Secondly, even after switching servers and datasets, we could still use Julia and \texttt{Flux.jl} to train our models. The issue here was the core support for multiple training provided by \texttt{Flux.jl}. Currently, only some third-party solutions exist, but most of them are outdated and incompatible with current versions of Julia and Flux. Instead of implementing these features ourselves, we checked out other frameworks. Due to the computational resources available internally at \acrshort{cgf}, we wanted to ensure that our code can be used at \acrshort{cgf}, no matter what resources they may obtain.

Additionally, since TinyDAS is open-source, we wanted to make sure researchers globally can train and compare the performance of models for anomaly detection. These considerations ultimately led us to Tinygrad instead of Pytorch. Tinygrads' hardware agnostic approach and lightweight framework enabled us to build on top of it and provide a scalable solution for any kind of accelerator. We have been pleasantly surprised by its features and how easy it is to enable half-precision training and data parallel training. We also made sure that TinyDAS can be used with our program Judas. By implementing functionality for reading smaller memory-mapped binary files and forking their own local copy of TinyDAS, members at \acrshort{cgf} can use Judas and TinyDAS together to allow for a direct and efficient pipeline between preprocessing and detecting anomalies in \acrshort{das} data as shown in Figure \ref{fig:judasnet_overview}. 

