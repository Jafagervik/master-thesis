\section*{Reflections on Julia}
\label{sec:juliaref}

Since we first started working on \texttt{Judas} \cite{projthesis}, we've continued working with Julia as the language of our choice. As the project has reached this point, we want to reflect on the positives and negatives by opting for Julia, compared to more commonly used languages such as Python, R, and MatLab \cite{matlabpyr}. While some previous points still stand \cite{projthesis}, some new points have occurred. \\

One of the main benefits of choosing Julia has been the speed it offers, which in many cases can be compared to that of C. From its \textit{Just ahead of time compilation} to its support for Meta programming \cite{whyjulia} \cite{julia}, there are several design choices that make Julia faster. Its type system is extremely rich, and due to its support for subtyping, novel programming design patterns have emerged that ... (See \ref{app:subtyping} for examples). \\ 

Most new languages and compilers come with a version multiplexer, examples being \texttt{rustup}, \texttt{zigup}. These are not built retrospectively, as is the case for \texttt{sdkman} for Java, but before. Managing dependencies and packages, maintaining larger programs and so on becomes much easier with a version multiplexer and a productive package manager. C has never had a standard way of dealing with packages, and this has inspired future languages to extend their ecosystems to include this alongside the compiler and standard library. Python has \texttt{PyPI}, but with many different programs to deal with versioning and packages. Some of these are venv, \texttt{Anaconda} and \texttt{Poetry}. Just simply installing and setting up projects in these languages seems to be harder than it actually has to be, and Julia proves this. \\ 


Although the language has several great benefits, it can still feel lacking in certain aspects. Compared to Python, it is still a rather new language, and its ecosystem has yet to grow into the titan the python ecosystem is. From the documentation to niche packages, Python still has the advantage and probably will for the coming years. This is especially relevant whe \\


 For \acrshort{ai} and \acrshort{ml} packages, we were pleasantly surprised to find multiple options that all perform well. Bindings to similar packages in Python could also easily be found. We opted for Flux, and with its native integration with CUDA, no extra work had to be done to use \acrshort{gpu}s to speed up the computation of our models. \\

Next to this comes the built-in \texttt{@inbounds} macro, which turns off the boundary checker when accessing memory, speeding up computations quickly. Not only this, but Julia's different macros were pleasant to work with. \texttt{time}, \texttt{btime}, \texttt{profile}, \texttt{cuda}, \texttt{btime}, \texttt{simd} all help immensely when creating programs, without the need for writing loops or custom instructions. Just knowing how and where to place macros cleans up the code and not only increases the developer experience but also standardizes code between codebases without having to rewrite all from scratch. Just simply running and launching cudakernels as in \ref{app:jlvsc} shows how easy it is to set up and run CUDA kernels as long as the \texttt{CUDA.jl} package is installed. \\

Enabling multiple processors or threads comes down to simply specifying a flag when running \texttt{-p} or \texttt{-t} respectively. The language has these kinds of computing built into the standard library, no need to install third party dependencies. \\ \\

Regarding AI, Python has remained the top choice for implementing and testing models. Tensorflow \cite{abadi2016tensorflow}, Pytorch \cite{paszke2019pytorch}, Jax \cite{47008}, and recently Tinygrad \cite{tinygrad} are all highly optimized frameworks for working with ML, some of them with thousands of articles, papers and learning materials written about them. Comparatively, Julias \texttt{Flux.jl} is way younger but offers, in our opinion, easier \acrshort{gpu} toggling, model design, and overall a smoother experience. The documentation of \texttt{Flux.jl} takes one through the entire code base, and after reviewing models on \href{https://github.com/FluxML/model-zoo}{github}, it's easy to figure out how to setup, train and save models.

Julia excels when it comes to scientific computations. Not only does the support of Unicode symbols make it easier to translate white papers to code, but Julia's syntax and its compilation make for an incredible developer experience, as well as increased performance. In the appendix, one can see an example of plotting in Julia, and how intuitive it can be, compared to other languages. \\
 
Although Julia has shown many strengths, there is nothing like a perfect programming language. Besides a far younger ecosystem compared to its alternatives, Julia's main weakness is its lack of documentation. \\

Another potential drawback is the relatively young ecosystem for \acrshort{ai} or \acrshort{ml} programming that exists compared to Python. As mentioned in \ref{chap:back}, Julia has bindings for TensorFlow, yet still \texttt{Flux.jl} is preferred in most cases. Although it seems to have all the functions necessary for computations, some of its functions are not as optimized as they can be. As discussed in \cite{projthesis}, \lstinline|relu| was not optimized until the end of last year and was quite slow. These optimizations, be they trivial or not, will impact larger programs significantly, so we might want to benchmark certain functions to see if they may be optimized further. However, this is also a great aspect of Julia. Due to how Flux is engineered, modifying or adding to the source code is still the way to go when looking at newer models. \\

The lack of core support for data-parallel training ultimately led us to use Python to implement our autoencoders. There do exist packages to provide tools for multiple training, but these are not part of \texttt{Flux.jl}, and some of these are outdated. This, and the lack of support for mixed precision, makes Julia ultimately not an optimal choice for \acrshort{ai} development yet, but support for multiple training is being discussed as a feature for the future.

\subsection*{JudasNET}
\label{disc:judasnet}

After the creation of Judas, we were determined to continue implementing of \acrshort{ai} models and anomaly detection in Julia. We wanted to train our models on the same dataset as described earlier \ref{met:judas}. Ultimately, there were two reasons for us turning away from Julia and opting for Python instead. \\
The first reason is the limited amount of \acrshort{gpu}s available at \acrshort{cgf}; with only a single consumer grade level \acrshort{gpu} with 11Gb VRAM available, we would have to use smaller batches of data and maybe decrease window duration. This constraint led us to switch to the \gls{idun} cluster for computation. Still, we were now not allowed to continue our work on the BANENOR dataset due to it being proprietary and classified data. IDUN is a public server, and most of the data at \acrshort{cgf} is not allowed on public servers. This led us to work with the PubDAS dataset. \\
Even after switching servers and datasets, we could still use Julia and \texttt{Flux.jl} to train our models. The issue here was the core support for multiple training provided by \texttt{Flux.jl}. The maintainers are discussing this issue \href{https://github.com/FluxML/Flux.jl/issues/1829}{in a forum}, but ultimately, only third-party solutions exist as of now. Several of these packages are old and incompatible with current Julia Flux versions. Instead of implementing this feature ourselves, we want to highlight how autoencoders can be used for anomaly detection on \acrshort{das} data. Thus, we ended up using Python. 

This work is collected in the \texttt{JudasNET} codebase and serves as a guideline for how further development.