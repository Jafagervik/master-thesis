\section*{Reflections on Julia}
\label{sec:juliaref}

Since we first started working on \texttt{Judas} \cite{projthesis}, we've continued working with Julia as the language of our choice. As the project have reached this point, we want to reflect on both the positives and negatives by opting for Julia, compared to more commonly used languages such as Python, R, and unfortunately MatLab \cite{matlabpyr}. While some of the points from before still stands \cite{projthesis}, some new points have occurred. \\

One of the main benefits to choosing Julia has been the speed it offers, which in many cases can be compared to that C. From its \textit{Just ahead of time compilation}, to its support for Meta programming \cite{whyjulia} \cite{julia}, there are several design choices that make Julia faster. Its typesystem is extremely rich, and due to its support for subtyping, novel programming design patterns have emerged that ... (See \ref{app:subtyping} for examples). \\ 

A majority of new languages and compilers comes with a version multiplexer, examples being \texttt{rustup}, \texttt{zigup}. These are not built retrospectively, as is the case for \texttt{sdkman} for Java, but before. Managing dependencies and packages, maintaining larger programs and so on becomes a lot easier with both a version multiplexer and a productive package manager. C has never had a standard way of dealing with packages, and this has inspired future languages to extend their ecosystems to include this alongside the compiler and standard library. Python has \texttt{PyPI}, but with many different programs to deal with versioning and packages. Some of these are venv, \texttt{Anaconda} and \texttt{Poetry}. Just simply installing and setting up projects in these languages seem to be harder than it actually have to be, and Julia proves this. \\ 


Although the language has several great benefits, it can still feel a bit lackluster in certain aspects. Compared to Python, it is still a rather new language, and its ecosystem has yet to grow into the titan the python ecosystem is. From documentation, to niche package, Python still has the advantage, and probably will for the coming years. This is specially relevant whe \\


 For \acrshort{ai} and \acrshort{ml} packages, we were pleasantly surprised to find multiple options that all perform well. Bindings to similar packages in Python could also easily be found. We opted for Flux, and with its native integration with CUDA, no extra work had to be done to make use of \acrshort{gpu}s to speed up the computation of our models. \\

Next to this comes the builtin \texttt{@inbounds} macro, which turns of the boundary checker when accessing memory, speeding up computations in a short matter of time. Not only this, but all the different macros in Julia were pleasant to work with. \texttt{time}, \texttt{btime}, \texttt{profile}, \texttt{cuda}, \texttt{btime}, \texttt{simd} all help imensly when creating programs, without the need for writing loops or custom instructions. Just simply knowing how and where to place macros cleans up the code, and not only increases the developer experience, but also standardizes code between codebases without having to rewrite all from scratch. Just simply running and launching cudakernels as in \ref{app:jlvsc} shows how easy it is to setup and run CUDA kernels as long as the \texttt{CUDA.jl} package is installed. \\

Enabling multiple processors or threads comes down to simply specifying a flag when running \texttt{-p} or \texttt{-t} respectively. The language has these kinds of computing built into the standard library, no need to install third party dependencies. \\ \\

When it comes to AI, Python has remained the top choice for implementing and testing models. Tensorflow \cite{abadi2016tensorflow}, Pytorch \cite{paszke2019pytorch}, Jax \cite{47008} and recently Tinygrad \cite{tinygrad} are all highly optimized frameworks for working with ML, some of them with thousands of articles, papers and learning materials written about them. Comparatively, Julias \texttt{Flux.jl} is way younger, but offers in our opinion, easier \acrshort{gpu} toggling, model design and overall a smoother experience for . The documentation of \texttt{Flux.jl} takes one through the entire code base, and after reviewing models on \href{https://github.com/FluxML/model-zoo}{github}, it's easy to figure out how to setup, train and save models.

Julia excels when it comes to scientific computations. Not only does the support of unicode symbols make it easier to translate white papers to code, but Julias syntax and its compilation makes for an incredible developer experience, as well as increased performance. In the appendix, one can see an example of plotting in Julia, and how intuitive it can be, compared to other languages. \\
 
Although Julia has shown to have many strengths, there is no thing such as a perfect programming language. Julia's main weaknesses besides a far younger ecosystem compared to its alternatives, is its lack of documentation. \\

Another potential drawback is the relatively young ecosystem for \acrshort{ai} or \acrshort{ml} programming that exits compared to Python. As mentioned in \ref{chap:back}, Julia has bindings for Tensorflow, yet still \texttt{Flux.jl} is preferred in most cases. Although it seems to have all functions necessary for computations, some of its functions are not as optimized as they can be. As discussed in \cite{projthesis}, \lstinline|relu| was not optimized until the end of last year, and was quite slow. These kinds of optimizations, be it trivial or not, will impact larger programs on a significant level, so we might want to benchmark certain functions to see if they may be optimized further. However, this is also a great aspect of Julia. Due to how Flux is engineered, modifying or adding to the source code is still the way to go when looking at newer models. \\

What ultimately led us to using Python for the implementation of our autoecoders was the lack of core support for data parallel training. There does exist packages to provide tools for multigpu training, but these are not part of \texttt{Flux.jl}, and some of these are outdated. This, and the lacking support of mixed precision, makes Julia ultimately not a optimal choice for \acrshort{ai} development yet, but support for multigpu training is being discussed as a feature for the future.