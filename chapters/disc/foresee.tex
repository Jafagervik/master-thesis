%Reducing model parameters, as discussed by \cite{s23021009}. 
\section{Case: FORESEE}
\label{disc:foresee}

\subsection{Experiment \rnum{1}: Model Training and Reconstruction}

As displayed in Table \ref{tab:modelresinfo}, the two convolutional models drastically reduce memory requirements, as expected. Our \acrshort{cae} model only needs 47k parameters, but still not close to the proposed solution by Tan et. al \cite{tan2023improving} using only 1.34k parameters. However, only needing \qty{97}{\si{\kilo\byte}} of memory is a great result, especially for real-time environments. The $\beta$-CVAE model requires some dense layers to represent the latent layer, which accounts for most parameters. Unlike these models, the other models are quite large, requiring more than \qty{5}{\si{\giga\byte}} of memory. The best losses are on the same scale for the variational and non-variational models due to the \acrshort{elbo} loss accounting for the \acrshort{kld}, making the loss higher. Additionally, the AE and CAE models converge fast, as displayed in Figure \ref{fig:losses} and highlighted in Table \ref{tab:modelresinfo}. 

One of the choices for using Tinygrad was due to the \acrshort{jit}-compilation. Due to this, we observed a great dropoff between the first and consecutive epochs, as seen in Figure \ref{fig:traintimes} for the non-variational models. However, the smaller difference between max and mean times for the variational autoencoders can be attributed to both implementation errors of the \acrshort{elbo} loss or other minute details. Furthermore, using half-precision for inference, we can drastically reduce the amount of time it takes to reconstruct a \acrshort{das} frame of 5 seconds, which is crucial for real-time scenarios. The inference times are close to the 3.3ms per sample achieved by Liu et al. \cite{photonics9100677}. The improvements in training times, accompanied by fast convergence, low memory requirements, and fast inference combined, are significant findings of several models. Although the highlighted statistics presented by Table \ref{tab:modelresinfo} and Figure \ref{fig:traintimes} present positive results, the models should capture finer details to effectively locate anomalies in these dense-sampled data. 

The reconstruction capabilities of the different models differ drastically, as shown in Figure \ref{fig:cae_reconstructions} and \ref{fig:beta_vae_reconstructions}. Out of the 4 models, the variational models produce seemingly random noise. In contrast, the AE model cannot capture finer details or, in layman's terms, ''only gets the average color of the picture''. As per the theory discussed in Section \ref{back:linear}, dense layers' limited feature extraction capabilities compared to convolutional differ greatly. Even though regular AE models have been proven to reconstruct smaller images such as the commonly used MNIST dataset, the existence of potential anomalous data in the trainset, the considerable difference in scale compared to regular image construction tasks, combined with our approach of drastically reducing dimensionality (as seen in Table \ref{tab:ae} in the appendix), might contribute to the subpar performance. The results might have been better if we had increased the number of hidden parameters by adding layers and allowing for a more gradual reduction in dimensionality. However, at the cost of increasing parameters and effective model size, we only use this model as a baseline to see how a relatively smaller and simpler model impacts anomaly detection. The inclusion of the AE model in our comparison was not to try to outperform networks of greater capabilities but rather serve as a baseline for how simple we can design models for this specific task and highlight the need for more feature extraction capabilities, as mentioned in Section \ref{back:cnn}.

As for the variational ones, the relatively small $\mu$ and $\sigma$ layers, as seen in Table \ref{tab:vae} and Table \ref{tab:cvae}, could indicate that the latent layers cannot even capture any details. The $\beta$ parameters are neither downplaying the importance of the \acrshort{kld}, which we set to stabilize the \acrshort{elbo} loss, further indicating the importance of larger latent layers. The validation losses seen in Figure \ref{fig:losses} indicate that this is not due to underfitting. We notice a certain difference between the two variational models, where the $\beta$-VAE can reconstruct certain parts of the heatmap, although nothing noteworthy. 

The clear standout here is our \acrshort{cae} model. Not only does it achieve the lowest reconstruction loss, as seen in Table \ref{fig:traintimes}, but the reconstructions are seemingly close to the original heatmaps. This, coupled with the small size of only 47k parameters and the low inference time of \qty{5.5}{\si{\milli\second}), results in a valuable model moving forward.

\subsection{Experiment \rnum{2}: Anomaly detection}

Given the experimental results in the last experiment, we now want to compare losses and the reconstructed heatmaps to the results found in this experiment. Our results reveal an intriguing paradox: the variational models ($\beta$-VAE and $\beta$-CVAE) excel in anomaly detection despite their poor reconstruction capabilities. This challenges our previous belief that better reconstruction would lead to better anomaly detection. 

The variational models might capture abstract features in their latent space $Z$ that are highly relevant for anomaly detection, even if these features don't translate to visually accurate reconstructions. This suggests that the essence of anomalies in our \acrshort{das} data might be more nuanced than initially assumed, which could definitely be the case for larger matrix data.

The superior performance of variational models across all metrics might indicate that our anomaly labeling, based on \cite{se-12-219-2021}, aligns more closely with abstract patterns rather than visually distinct features. On the contrary, a biased approach to labeling might have been the root course of this. Labeling \acrshort{das} data requires domain expertise, and our approach aimed at interpreting results found by et al. \cite{se-12-219-2021}. The reliance on potentially biased labeling might have impacted our anomaly scores. However, we chose a small number of files and spent significant time identifying each frame as accurately as possible. Another potential for this research would have been to use an un
We ultimately believe there is some truth in both that the size of the latent layer is way too small for our window duration.

The higher reconstruction losses of variational models and their better anomaly detection suggest these models might be learning more generalized data representations. This could be useful in identifying novel anomalies that don't necessarily manifest in the reconstructed heatmaps, as shown in Figure \ref{fig:beta_vae_reconstructions} and Figure \ref{fig:beta_cvae_reconstructions}. This highlights a potential for further research about the aspect of non-traditional representations of \acrshort{das} data.

Our models' low FPR, excluding AE, is noteworthy. False alarms can be costly and disruptive in real-world \acrshort{das} applications, as discussed in Section \ref{back:anomdet}. Our models' ability to maintain low FPR while achieving high detection accuracy is a significant strength. Additionally, a relative high precision of over 90\% is relatively good.

Our results demonstrate that the criteria for selecting anomaly detection models in DAS applications should go beyond just reconstruction accuracy. Factors like latent space representation, generalization capability, and practical deployment considerations play crucial roles.

The performance discrepancy between models raises questions about optimal strategies for labeling anomalies in DAS data. Future work might explore unsupervised or semi-supervised approaches that rely less on predefined visual criteria.

Overall, we presented a case for smaller autoencoders for anomaly detection. Although not state-of-the-art, we believe that exploring smaller convolutional autoencoders is a valid approach, especially in resource-constrained environments. Our \acrshort{cae} model stands as a clear representation of this. The combination of low inference time per sample, small model size, high reconstructive capabilities, and good scores overall, especially the low FPR. Furthermore, the use of half-precision inference has yielded an interesting point of discussion.

Even though only one of our four models yields positive reconstructive capabilities, analyzing and studying the importance of careful model design has been insightful. 
even though not capturing the temporal characteristics as many of the hybrid models do \ . 

