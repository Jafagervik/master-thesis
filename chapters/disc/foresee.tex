\section{FORESEE}
\label{disc:foresee}

\subsection{Experiment \rnum{1}: Model Training and Reconstruction}

As seen in Table \ref{tab:modelresinfo}, the two convolutional models drastically reduce memory requirements. These models only require . 
Reducing model parameters, as discussed by \cite{s23021009} . 

To further reduce memory consumption, half-precision or even mixed-precision training could be introduced to reduce the overall model and batch data size, and speed up training. Distributed dataparallel training could also be introduced, to distribute \acrshort{cpu} requirements, where each \acrshort{gpu} is owned by a \acrshort{cpu}.

The highest epoch durations, as displayed in Figure \ref{fig:traintimes} always happen on the first epoch due to \texttt{Tinygrad}s \textit{just-in-time} compilation, as discussed in Section \ref{meth:tinyoverview}. 


asd \ref{fig:losses}. 
      
The reconstruction capabilities of the different models differ drastically as per Figure \ref{fig:aereconstruct}). As per the theory discussed in Section \ref{back:linear}, the limited feature extraction capabilities of dense layers compared to convolutional proves big, especially for \acrshort{das} data. This is highlighted by how the AE model is able to capture the average values across the signal but fails to capture essential features. The inclusion of the AE model in our comparison was not to try to outperform networks of greater capabilities, but rather underscore the importance of context when constructing \acrshort{das}. We do not 

The $\beta-VAE$ mo

Even though we have highlighted the \textit{spatio-temporal} aspects of \acrshort{das} data, the selected models for comparisons try to underscore  \acrshort{das} data. 

\subsection{Experiment \rnum{2}: Anomaly detection}

Failed models? 
\subsection{Limitations} 

Dataset
Float16