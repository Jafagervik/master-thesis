\section{Case: FORESEE}

\subsection{Dataset}

For our second case, we use the \acrshort{foresee} dataset from the the PubDAS \cite{spica2023pubdas} collection. PubDAS ("A PUBlic Distributed Acoustic Sensing Datasets Repository for Geosciences") contains \acrshort{das} data from numerous locations worldwide. We will specifically be dealing with the \acrshort{foresee} \cite{zhu2019penn, se-12-219-2021} array, a \acrshort{das} dataset from an area around Pennsylvania in the Valley and Ridge Appalachians region as seen in figure \ref{fig:foresee}. \\

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/foresee.png}
    \caption{Map of the FORESEE Array. Source: \cite{spica2023pubdas}}
    \label{fig:foresee}
\end{figure}

\subsubsection{Data Preprocessing}
\label{exp:fordata}

\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{@{}p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}@{}}
        \toprule
        \textbf{Parameter} & \multicolumn{2}{l}{\textbf{Value}} \\
        \midrule
        Experiment & \multicolumn{2}{l}{Foresee}  \\
        Interrogator Unit (IU) & \multicolumn{2}{l}{Silixa iDAS-v2}  \\
        Gauge length & \multicolumn{2}{l}{\qty{10}{\si{\meter}}} \\
        Cable length & \multicolumn{2}{l}{\qty{23300}{\si{\meter}}} \\
        Channel spacing & \multicolumn{2}{l}{\qty{2}{\si{\meter}}} \\
        \midrule
        & \textbf{Original Data} & \textbf{After Preprocessing} \\
        \cmidrule(lr){2-3}
        Format & TDMS & HDF5 \\
        Sample rate & \qty{500}{\si{\hertz}} & \qty{125}{\si{\hertz}} \\
        Time window & \qty{10}{\si{\minute}} & \qty{5}{\si{\second}} \\
        Data shape & 300000 \(\times\) 2137 & 625 \(\times\) 2137 \\
        Datatype & Float32 & Float32 \\
        File size & \qty{2.5644}{\si{\giga\byte}} & \qty{4.98}{\si{\mega\byte}} \\
        \midrule
        \textbf{Dataset Information} & \multicolumn{2}{l}{} \\
        Train dataset size & \multicolumn{2}{l}{\qty{25690}{files}} \\
        Train dataset span & \multicolumn{2}{l}{2020-03-02 08:10:15 to 2020-03-03 20:40:10} \\
        Anomalous dataset size & \multicolumn{2}{l}{\qty{600}{files}} \\
        Anomalous dataset span & \multicolumn{2}{l}{2019-04-15 03:17:35 to 2019-04-15 04:07:30} \\
        \bottomrule
    \end{tabular}
    \caption{\acrshort{foresee} Experiment Data Summary}
    \label{tab:foresee_experiment_data}
\end{table}

Preprocessing of the dataset has already been performed, as detailed in Appendix \ref{app:pubdas}. The files have been converted to the \acrshort{hdf5} format and the data resampled. Due to memory constraints of most consumer-grade \acrshort{gpu}s, which typically have around 8-32 GB of VRAM, the number of 10-minute data batches that can be processed simultaneously is limited. This constraint is worsened by the need to further store the model's weights, biases, and computed losses in VRAM.
To address these limitations and enhance the system's responsiveness, we chose to split the files into 5-second segments instead of the original 10-minute recordings. This approach offers two major advantages:

\begin{itemize}
    \item It allows for training across various GPU architectures, accommodating different memory capacities.
    \item It enables more frequent updates in online anomaly detection, potentially identifying anomalous events every 5 seconds, dramatically improving the system's real-time response capability.
\end{itemize}


The file-splitting process was implemented in parallel using Julia, resulting in a collection of nearly 26,000 files for our training dataset. This preprocessing strategy mitigates hardware constraints and aligns with the requirements of real- or near-real-time anomaly detection systems, where rapid identification of anomalies is crucial. A crucial aspect of our approach concerns the composition of the training dataset:

\begin{enumerate}
    \item \textit{Precense of anomalies}: The training dataset \textit{may} contain some anomalous data. This deliberate decision reflects real-world scenarios where completely ``clean'' data is rare.
    \item \textit{Time period selection}: We attempt to mitigate this by selecting a time period with minimal documented extreme weather events, earthquakes, or other known anomalous occurrences. This approach aims to minimize, but not entirely eliminate, the presence of anomalies in the training data.
    \item \textit{Model robustness}: By including potential anomalies in the training set, we aim to examine whether our models are sufficiently robust to withstand and learn from data that may contain a small proportion of anomalous samples.
    \item \textit{Semi-supervised Anomaly Detection}: Although labeled anomalies technically place our approach in the realms of self-supervised anomaly detection, our goal is to develop models that can effectively identify anomalies despite this challenge.
\end{enumerate}

It's important to note that while data quality and quantity are paramount in deep learning, we've consciously decided to balance preprocessing effort with practical considerations. Our approach aims to reduce the time and resources spent on extensive preprocessing, reflecting real-world constraints where perfect data cleaning may not always be feasible or economically viable. Information about the data can be found in Table \ref{tab:foresee_experiment_data}. 


\subsection{Experiment \rnum{1}: Model training and Reconstruction}


In this first experiment, we train four different autoencoders and examine their reconstruction capabilities, focusing on reconstruction loss, median- and overall training time. These four models consist of a dense regular autoencoder (\acrshort{ae}), a $\beta$-variational autoencoder ($\beta$-VAE), a convolutional autoencoder (\acrshort{cae}) and finally a $\beta$-variational convolutional model ($\beta$-CVAE). Appendix \ref{app:archs} details all the layers and parameter numbers.

As seen in Table \ref{tab:foresee_experiment_data}, the memory required for a single \acrshort{hdf5} file is still quite large compared to regular image- and video-based tasks, where the dimensions typically. For the dense autoencoders, this poses a challenge when deciding on the dimensions of the hidden layers. Our approach is to drastically reduce the dimension from the input- to the first hidden layers. This 

\begin{table}[!h]
\centering
\small
\rowcolors{2}{gray!15}{white} % Start with the second row, alternate between light gray and white
\begin{tabular}{@{}lllll@{}}
\toprule
\rowcolor{gray!30} % Darker gray for the header row
\textbf{Parameter} & \textbf{AE} & \textbf{$\beta$-VAE} & \textbf{CAE} & \textbf{$\beta$-CVAE} \\
\midrule
Files & 25600 & 25600 & 25600 & 25600 \\
Input Shape & [BS, 625 × 2137] & [BS, 625 × 2137] & [BS, 1, 625, 2137] & [BS, 1, 625, 2137] \\
Batch Size & 256 & 256 & 32 & 32 \\
Validation Split & 0.2 & 0.2 & 0.2 & 0.2 \\
Epochs & 200 & 200 & 200 & 200 \\
Seed & 1337 & 1337 & 42069 & 1337 \\
Loss Function & \acrshort{mse} & \acrshort{elbo} ($\beta=0.8$) & \acrshort{mse} & \acrshort{elbo} ($\beta=0.5)$ \\
Optimizer & ADAM & ADAM & ADAM & ADAM \\
Learning Rate & 0.001 & 0.001 & 0.002 & 0.001 \\
LR Scheduler & ReduceOnPlateau & ReduceOnPlateau & ReduceOnPlateau & ReduceOnPlateau \\
LR Scheduler Patience & 5 & 5 & 5 & 5 \\
LR Scheduler Factor & 0.5 & 0.5 & 0.5 & 0.5 \\
Early Stopping Patience & 10 & 10 & 10 & 10 \\
Early Stopping Min $\Delta$ & 0.0001 & 0.0001 & 0.0001 & 0.001 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for all models}
\label{tab:hyperparameters}
\end{table}
As shown in Table \ref{tab:hyperparameters}, the model sizes differ quite drastically, which aligns with our expectations based on their architectures. Convolutional layers typically require fewer parameters than linear layers, resulting in convolutional autoencoders needing less memory. However, we need to decrease the batch size for convolutional models since each convolutional layer operates on higher-dimensional data, increasing GPU memory usage during training. Additionally, the \acrshort{vae} models require extra parameters since the latent layer is represented by both $\mu$ and $\sigma$ layers, slightly increasing their size compared to their non-variational counterparts. Detailed descriptions of all the models and their layer structures can be found in Appendix \ref{app:archs}. 

 The relatively large batch sizes (256 for linear models, 32 for convolutional) allow for stable gradient updates. At the same time, the learning rate of 0.001 is a common starting point for the \acrshort{adam} optimizer \cite{kingma2017adam}. To optimize the training process, we introduce a learning rate scheduler that reduces the learning rate by half if the validation loss does not decrease over 5 epochs. Combined with an early-stop mechanism, we can attempt escaping local minima and stop unnecessary training whenever the validation loss does not decrease by at least $\Delta$ (0.0001) by 10 consecutive epochs.  

To evaluate and compare the reconstruction capabilities of these models, we will primarily use the \acrshort{mse} for the AE and CAE, and  \acrshort{elbo} loss with \acrshort{mse} for $\mathcal{L}_{rec}$ for the $\beta$-VAE and $\beta$-CVAE, as described in section \ref{back:elbo}. We will also visually inspect reconstructed samples to qualitatively assess the models' performance.



\subsection{Experiment \rnum{2}: Semi-Supervised Anomaly Detection}

For this experiment, we select a test dataset for labeling anomalies based on the findings of Zhu et al. \cite{zhu2023seismic}, who identified 18 thunder-induced seismic events between 03:20 and 03:50 on the 15th of April 2019. We manually label each heatmap as either anomalous or non-anomalous. As we discussed in \ref{back:relwork}, many of the different studies on \acrshort{das} data concern themselves with specific events they target, such as earthquakes. Researchers have identified certain characteristics of anomalous data for the \acrshort{foresee} dataset as mentioned in Section \ref{back:relwork}. Labeling some data allows for finding a potentially better threshold; what we ultimately want in a real-world scenario is to flag potential anomalies with as high accuracy as possible, as seen in Algorithm \ref{alg:ad}. Our approach can thus be described as unsupervised training with semi-supervised evaluation. To assess model performance, we will compare the following metrics for both Float32 and Float16 inference:

\subsection{Experiment Setup}

Model training and anomaly detection are performed on \gls{idun}. Table \ref{tab:system-specs} in Appendix \ref{app:idun} shows details of the machines used. \\

