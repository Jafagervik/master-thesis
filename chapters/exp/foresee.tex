\section{FORESEE}

\subsection{Dataset}

In this experiment, we use datasets from the PubDAS \cite{spica2023pubdas} collection. PubDAS is described as "A PUBlic Distributed Acoustic Sensing Datasets Repository for Geosciences" and contains \acrshort{das} data from numerous locations worldwide. We will specifically be dealing with the FORESEE \cite{zhu2019penn} dataset, a \acrshort{das} dataset from an area around Pennsylvania in the Valley and Ridge Appalachians region as seen in figure \ref{fig:foresee}. \\

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/foresee.png}
    \caption{Map of the FORESEE Array. Photo is taken the PubDAS paper \cite{spica2023pubdas}}
    \label{fig:foresee}
\end{figure}

\subsubsection{Data Preprocessing}
\label{exp:fordata}

\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{@{}p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}@{}}
        \toprule
        \textbf{Parameter} & \multicolumn{2}{l}{\textbf{Value}} \\
        \midrule
        Experiment & \multicolumn{2}{l}{Foresee}  \\
        Interrogator Unit (IU) & \multicolumn{2}{l}{Silixa iDAS-v2}  \\
        Gauge length & \multicolumn{2}{l}{\qty{10}{\si{\meter}}} \\
        Cable length & \multicolumn{2}{l}{\qty{23300}{\si{\meter}}} \\
        Channel spacing & \multicolumn{2}{l}{\qty{2}{\si{\meter}}} \\
        \midrule
        & \textbf{Original Data} & \textbf{After Preprocessing} \\
        \cmidrule(lr){2-3}
        Format & TDMS & HDF5 \\
        Sample rate & \qty{500}{\si{\hertz}} & \qty{125}{\si{\hertz}} \\
        File duration & \qty{10}{\si{\minute}} & \qty{5}{\si{\second}} \\
        Data shape & 300000 \(\times\) 2137 & 625 \(\times\) 2137 \\
        Datatype & Float32 & Float32 \\
        File size & \qty{2.5644}{\si{\giga\byte}} & \qty{4.98}{\si{\mega\byte}} \\
        \midrule
        \textbf{Dataset Information} & \multicolumn{2}{l}{} \\
        Train dataset size & \multicolumn{2}{l}{\qty{25690}{files}} \\
        Train dataset span & \multicolumn{2}{l}{02 Mar 2020 08:10:15 to 03 Mar 2020 20:40:10} \\
        Anomalous dataset size & \multicolumn{2}{l}{\qty{600}{files}} \\
        Anomalous dataset span & \multicolumn{2}{l}{15 Apr 2019 03:17:35 to 15 Apr 2019 04:07:30} \\
        \bottomrule
    \end{tabular}
    \caption{FORESEE Experiment Data Summary}
    \label{tab:foresee_experiment_data}
\end{table}

Preprocessing of the dataset has already been performed, as detailed in Appendix \ref{app:pubdas}. The files have been converted to the \acrshort{hdf5} format and the data resampled. Due to memory constraints of most consumer-grade \acrshort{gpu}s, which typically have around 8-32 GB of VRAM, the number of 10-minute data batches that can be processed simultaneously is limited. This constraint is worsened by the need to further store the model's weights, biases, and computed losses in VRAM.
To address these limitations and enhance the system's responsiveness, we chose to split the files into 5-second segments instead of the original 10-minute recordings. This approach offers two major advantages:

\begin{itemize}
    \item It allows for training across various GPU architectures, accommodating different memory capacities.
    \item It enables more frequent updates in online anomaly detection, with the potential to identify anomalous events every 5 seconds, dramatically improving the system's real-time response capability.
\end{itemize}


The file-splitting process was implemented in parallel using Julia, resulting in a collection of nearly 26,000 files for our training dataset.This preprocessing strategy not only mitigates hardware constraints but also aligns with the requirements of real-time or near-real-time anomaly detection systems, where rapid identification of anomalies is crucial. A crucial aspect of our approach concerns the composition of the training dataset:

\begin{enumerate}
    \item \textit{Precense of anomalies}: The training dataset \textit{may} contain some anomalous data. This deliberate decision reflects real-world scenarios where completely ``clean'' data is rare.
    \item \textit{Time period selection}: We attempt to mitigate this by selecting a time period with minimal documented extreme weather events, earthquakes, or other known anomalous occurrences. This approach aims to minimize, but not entirely eliminate, the presence of anomalies in the training data.
    \item \textit{Model robustness}: By including potential anomalies in the training set, we aim to examine whether our models are sufficiently robust to withstand and learn from data that may contain a small proportion of anomalous samples.
    \item \textit{Unsupervised learning}: Although the presence of unlabeled anomalies technically places our approach in the realm of unsupervised learning, our goal is to develop models that can effectively identify anomalies despite this challenge.
\end{enumerate}

It's important to note that while data quality and quantity are paramount in deep learning, we've made a conscious decision to balance preprocessing effort with practical considerations. Our approach aims to reduce the time and resources spent on extensive preprocessing, reflecting real-world constraints where perfect data cleaning may not always be feasible or economically viable. Information about the data can be found in table \ref{tab:foresee_experiment_data}. \\

Finally, we select a test dataset for labeling anomalies based on the findings of Zhu et al. (2023) \cite{zhu2023seismic}, who identified 18 thunder-induced seismic events between 03:20 and 03:50 on the day in question. We manually label each file as either anomalous or non-anomalous. This labeled dataset enables us to calculate confusion matrices and other relevant metrics to evaluate the accuracy of our autoencoders. Our approach can thus be described as unsupervised learning with supervised evaluation.

%TODO: Inference data from 15042019!


\subsection{Experiment \rnum{1}: Model training and Reconstruction}

\begin{table}[!h]
\centering
\begin{tabular}{@{}l*{4}{l}@{}}
\toprule
\textbf{Parameter} & \textbf{AE} & \textbf{VAE} & \textbf{CAE} & \textbf{CVAE}\\
\midrule
Files & \multicolumn{4}{c}{25600} \\
Input Shape & \multicolumn{2}{c}{$[\text{Batch Size}, 625 \times 2137]$} & \multicolumn{2}{c}{$[\text{Batch Size}, 1, 625, 2137]$} \\
Batch Size & \multicolumn{2}{c}{256} & \multicolumn{2}{c}{32} \\
Validation Split & \multicolumn{4}{c}{0.2} \\
Epochs & \multicolumn{4}{c}{200} \\
Seed & \multicolumn{4}{c}{1337} \\
Optimizer & \multicolumn{4}{c}{ADAM} \\
Learning rate & \multicolumn{4}{c}{0.001} \\
LR Scheduler & \multicolumn{4}{c}{ReduceLROnPlateau} \\
LR Scheduler Patience & \multicolumn{4}{c}{5} \\
LR Scheduler Factor & \multicolumn{4}{c}{0.5} \\
Early stopping patience & \multicolumn{4}{c}{10} \\
Early stopping min $\Delta$ & \multicolumn{4}{c}{0.0001} \\
Loss function & \acrshort{mse} & \acrshort{elbo} & \acrshort{mse} & \acrshort{elbo} \\
Layer dimensions & \multicolumn{2}{c}{$[1024, 512, 128]$}  & \multicolumn{2}{c}{[1, 16, 8, 8]}\\
Model Sizes & \qty{2.71}{\giga\byte} & \qty{0.01159}{\mega\byte} & \qty{5.183}{\giga\byte} & \qty{0.335}{\giga\byte} \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for all models}
\label{tab:hyperparameters}
\end{table}

In this first experiment, we train four different autoencoders and examine their reconstruction capabilities, focusing on reconstruction loss, median- and overall training time. These four models are a dense regular autoencoder (AE \ref{app:a-ae}), a variational autoencoder (VAE \ref{app:a-vae}), and convolutional variants of both (CAE \ref{app:a-cae} and CVAE \ref{app:a-cvae}). \\ 

As shown in table \ref{tab:hyperparameters}, the model sizes differ quite drastically, which aligns with our expectations based on their architectures. Convolutional layers typically require fewer parameters than linear layers, resulting in convolutional autoencoders needing less memory. However, we need to decrease the batch size for convolutional models since each convolutional layer operates on higher-dimensional data, increasing GPU memory usage during training. Additionally, the \acrshort{vae} models require extra parameters since the latent layer is represented by both $\mu$ and $\sigma$ layers, slightly increasing their size compared to their non-variational counterparts. Detailed descriptions of all the models and their layer structures can be found in Appendix \ref{app:archs}. \\

We selected these hyperparameters based on a combination of literature review and preliminary experiments. The relatively large batch sizes (256 for dense models, 32 for convolutional) allow for stable gradient updates, while the learning rate of 0.001 is a common starting point for the \acrshort{adam} optimizer \cite{kingma2017adam}. To optimize the training process, we introduce a learning rate scheduler that reduces the learning rate by half if the validation loss does not decrease over 5 epochs. Combined with an early-stop mechanism described in the section, we can attempt escaping local minima and stop unnecessary training whenever the validation loss does not decrease by at least $\Delta$ (0.0001) by 10 consecutive epochs. \\ 

To evaluate and compare the reconstruction capabilities of these models, we will primarily use the \acrshort{mse} for the AE and CAE, and  \acrshort{elbo} loss with \acrshort{mse} for $\mathcal{L}_{rec}$ for the VAE and CVAE, as described in section \ref{back:elbo}. We will also visually inspect reconstructed samples to assess the models' performance qualitatively.



\subsection{Experiment \rnum{2}: Anomaly Detection}
Without a real-time feed of \acrshort{das} data, we use a labeled dataset to evaluate the different models' capabilities for anomaly detection, as described in section \ref{exp:fordata}. From this, we compute a confusion matrix and evaluate the models based on the following metrics:

\begin{itemize}
    \item Precision: The percentage of correct anomalies out of all predicted anomalies.
    \begin{equation}
        Precision = \frac{TP}{TP + FP}
    \end{equation}

    \item True Positive Rate (TPR), also known as recall: The percentage of correctly identified anomalies out of all actual anomalies.
    \begin{equation}
        TPR = Recall = \frac{TP}{TP + FN}
    \end{equation}

    \item F1-score: Used to evaluate the balance between precision and recall; the higher the score, the better. 
    \begin{equation}
        F1_{score} = 2 \times \frac{Precision \cdot Recall}{Precision + Recall}
    \end{equation}

    \item False Positive Rate (FPR): The percentage of normal windows incorrectly identified as anomalous. FPR is highly relevant for \acrshort{das} systems, where false alarms may be costly. A low FPR indicates that normal data is not misclassified as anomalous; thus, the lower the score, the better.
    \begin{equation}
        FPR = \frac{FP}{FP + TN}
    \end{equation}
\end{itemize}

In addition to these metrics, we will also use the \acrfull{pr} curve and the \acrshort{pr} \acrfull{auc}. The PR curve is particularly useful for imbalanced datasets, which are common in anomaly detection tasks.

As seen in Figure~\ref{fig:dataflow}, we ultimately want to find a good threshold score ($\epsilon$) for our models. To determine the optimal threshold, we will analyze how precision, recall, and F1-score change across different threshold values, thus identifying the best anomaly score threshold.

\subsection{Experiment Setup}

All the models were trained and tested on \gls{idun} computers made for \acrshort{hpc}. Details of the machines used can be found in the table below. \\


\begin{table}[!htbp]
\centering
\caption{Specifications for Model Training and Testing Environment}
\label{tab:system-specs}
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Component} & \textbf{Description} & \textbf{Quantity} \\
\midrule
Operating System & Ubuntu Linux 22.04 LTS & 1 machine \\
GPU Model & NVIDIA H100 PCIe & 4 units \\
GPU Memory & HBM3 & 80 GB per GPU \\
CUDA Cores & & 14,592 per GPU \\
Tensor Cores & & 576 per GPU \\
GPU Clock Speed & Boost Clock & 1.67 GHz \\
GPU TDP & & 350 W \\
FP16 & & 204.9 TFLOPS \\
FP32 & & 51.22 TFLOPS \\
\midrule
\multicolumn{3}{@{}l@{}}{\textit{Note:} 1 \acrshort{gpu} dedicated for testing, 4 for training.} \\
\bottomrule
\end{tabular}
\end{table}