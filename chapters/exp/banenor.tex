\section{BANENOR}

\subsection{Dataset}

Our first experiment revolves around a \acrshort{das} dataset on a train route between Trondheim and Storen and is owned by BANENOR. The dataset spans the entirety of the 31st of August 2021\footnote{Working with national infrastructure requires security clearance; see \ref{app:conf} for more details}. The full route between Trondheim and Storen can be seen in appendix \ref{app:judas}. All the data is stored in \acrshort{hdf5} files.

\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{@{}p{0.3\textwidth}p{0.4\textwidth}@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Experiment & 210830\_NTNU\_Bane\_NOR\_GL8De4F2000  \\
        File timestamp & 2021-08-31 10:00:01  \\
        Type of data & Phase rate per distance (rad/m/s) \\
        Sampling frequency & \qty{2000}{\hertz} \\
        Window duration & \qty{10}{\second} \\
        Channel distance & \qty{4.0852}{\meter} \\
        \midrule
        Data shape & 20000 samples \(\times\) 12500 channels  \\
        \midrule
        Gauge length & \qty{8.1704}{\meter} \\
        Sensitivities & \qty{9.3622e6}{\radian} \\
        Regions of interest (ROI) & 1:4:49996 \\
        \bottomrule
    \end{tabular}
    \caption{BANENOR Experiment Data Summary from a single \acrshort{hdf5} file recorded on August 31, 2021 at 10:00 AM. The ROI parameter indicates that every 4th sensor's data is stored.}
    \label{tab:experiment_data}
\end{table}

Table \ref{tab:experiment_data} presents a summary of the experimental dataset, encompassing approximately \qty{50000}{\meter} of fiber-optic cable. Data acquisition occurs regularly along the route, with every fourth sensor's data retained, resulting in an effective inter-channel distance of \qty{4.0852}{\meter}. Each \acrshort{hdf5} file contains \qty{10}{\second} of recordings, represented as a matrix of dimensions $20000 \times 12500$, accompanied by important metadata. The element type of the matrix is \texttt{Float32}, yielding a base file size of approximately \qty{0.93}{\giga\byte}.

\subsection{Experiment \rnum{1}: Processing \acrshort{das} Files}
A fundamental challenge for members at \acrshort{cgf} involves efficiently retrieving and loading \acrshort{das} files from storage media for subsequent processing and analysis. This experiment assesses the performance of loading varying data volumes into memory using multiple parallel processes. The complete experiment code is detailed in Appendix \ref{app:judas}.

We set the step parameter to 48 to enhance processing efficiency, effectively modifying the Region of Interest (ROI) to 1:192:49996. While reducing spatial resolution, this adjustment facilitates faster processing and analysis of extensive datasets. We evaluate processing times for \acrshort{das} data spanning 5, 10, 30, and 60 minutes to assess scalability and performance across diverse data volumes. The experiment uses 1, 2, 4, and 8 parallel processes for comparative analysis. The experiment covers finding and loading files, downsampling the data to \qty{200}{\hertz}, and finally denoising the signal.

\subsection{Experiment \rnum{2}: Parallel Resampling}
Channel decimation, alternatively termed downsampling, represents another critical operation in \acrshort{das} data processing, as mentioned in Section \ref{back:dsp}. This experiment compares our parallel resampling method, shown in Code Listing \ref{code:parres}, with a conventional serial approach. The input dataset comprises a \qty{30}{\minute} \acrshort{das} file, maintaining consistent channel spacing with Experiment \rnum{1}. We perform signal resampling from the original \qty{2000}{\hertz} to a series of lower frequencies: \qty{1000}{\hertz}, \qty{500}{\hertz}, \qty{250}{\hertz}, and \qty{100}{\hertz}. The experiment utilizes the same process configurations (1, 2, 4, and 8 parallel processes) as described in Experiment \rnum{1}.

\subsection{Evaluation Metrics and Setup}

We use speedup and efficiency as evaluation metrics for both experiments. Speedup ($S_p$) is defined as the ratio between the serial execution time ($T_1$) and the parallel execution time ($T_p$) and can be formulated as:
\begin{equation}
    S_p = \frac{T_1}{T_p}
\end{equation}

Efficiency is the speedup ($S_p$) divided by the number of processes used $p$: 
\begin{equation}
    E_p = \frac{S_p}{p} = \frac{T_1}{p T_p}
\end{equation}

We perform all benchmarks on local servers belonging to \acrshort{cgf}. The system specifications are all listed in the table below \ref{tab:cgfsetup}. \\

\begin{table}[!h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Details} \\
\midrule
Operating System & Ubuntu Linux & Version 20.04 LTS \\
Processor & Intel Core i9-9940X & \qty{4.40}{\giga\hertz} \\
RAM & 126 GB & DDR4-2400 MHz \\
GPU & NVIDIA GeForce RTX 2080 Ti &  \qty{11}{\giga\byte} GDDR6 \\
\bottomrule
\end{tabular}
\caption{System Specifications for Experimental Setup}
\label{tab:cgfsetup}
\end{table}