\section{Experiment 1: BANENOR}

\subsection{Dataset}

Our first experiment revolves around a \acrshort{das} dataset on a train route between Trondheim and Storen and is owned by BANENOR. The dataset spans the entirety of the 31st of August 2021\footnote{Working with national infrastructure requires security clearance; see \ref{app:conf} for more details}. The full route between Trondheim and Storen can be seen in appendix \ref{app:judas}. All the data is stored in \acrshort{hdf5} files.

\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{@{}p{0.3\textwidth}p{0.4\textwidth}@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Experiment & 210830\_NTNU\_Bane\_NOR\_GL8De4F2000  \\
        File timestamp & 2021-08-31 10:00:01  \\
        Type of data & Phase rate per distance (rad/m/s) \\
        Sampling frequency & \qty{2000}{\si{\hertz}} \\
        Window duration & \qty{10}{\si{\second}} \\
        Channel distance & \qty{4.0852}{\si{\meter}} \\
        \midrule
        Data shape & 20000 samples \(\times\) 12500 channels  \\
        \midrule
        Gauge length & \qty{8.1704}{ \si{\meter}} \\
        Sensitivities & \qty{9.3622e6}{\si{\radian}
        }\\
        Regions of interest (ROI) & 1:4:49996 \\
        \bottomrule
    \end{tabular}
    \caption{BANENOR Experiment Data Summary from a file recorded the 31st of august 2021 at 10am}
    \label{tab:experiment_data}
\end{table}


As shown in Table \ref{tab:experiment_data}, the total distance covered by this dataset is approximately \qty{50000}{\meter}. Data is collected from sensors spaced at regular intervals along the route, with every fourth sensor's data stored. Each data file contains \qty{10}{\second} of recordings, stored as a matrix of size $20000 \times 12500$, alongside relevant metadata. Each matrix element is of type \texttt{Float32}, resulting in a base data size of approximately \qty{0.93}{\giga\byte} per file. \\

\subsection{Experiment \rnum{1}: Finding and Loading \acrshort{das} files}

One of the most common tasks for members at \acrshort{cgf} is to load \acrshort{das} files from disk for further processing or analysis. We conduct a full test here, loading different amounts of files into memory using different amounts of processes. The experiment code can be found in appendix \ref{app:judas}.

\subsection{Experiment \rnum{2}: Parallel Resampling}

Another common operation is performing channel decimation, also known as desampling, as described in section \ref{back:dsp}. We compare our parallel resampling method described in section \ref{code:parres}) to a serial approach. The input data will be a \qty{10}{\si{\minute}} \acrshort{das} data matrix, where the channel distance will be about 200m.

\subsection{Evaluation Metrics and Setup}

We use speedup and efficiency as evaluation metrics for both experiments. Speedup ($S_p$) is defined as the ratio between the serial execution time ($T_1$) and the parallel execution time ($T_p$) and can be formulated as:
\begin{equation}
    S_p = \frac{T_1}{T_p}
\end{equation}

Efficiency is the speedup ($S_p$) divided by the number of processes used: 
\begin{equation}
    E_p = \frac{S_p}{p} = \frac{T_1}{p T_p}
\end{equation}

We perform all benchmarks on local servers belonging to \acrshort{cgf}. The system specifications are all listed in the table below \ref{tab:cgfsetup}. \\

\begin{table}[!h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Details} \\
\midrule
Operating System & Ubuntu Linux & Version 20.04 LTS \\
Processor & Intel Core i9-9940X & \qty{4.40}{\giga\hertz} \\
RAM & 126 GB & DDR4-2400 MHz \\
GPU & NVIDIA GeForce RTX 2080 Ti &  \qty{11}{\giga\byte} GDDR6 \\
\bottomrule
\end{tabular}
\caption{System Specifications for Experimental Setup}
\label{tab:cgfsetup}
\end{table}