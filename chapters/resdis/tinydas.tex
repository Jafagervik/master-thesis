\section{TinyDAS}
\label{res:tinydas}

In this section, we'l be looking at the results for both training our autoencoders, as well as testing our models on the test data mentioned in the method section.

\subsection{Result Setup}

All the models were trained and tested on \gls{idun} computers made for \acrshort{hpc}. Configuration parameter for the different models can be found in the appendix \ref{app:judasnethyperparams}, and details of the machines used can be found in the table below. \\



\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Unit} & \textbf{Description}         \\ \hline
Os            & Ubuntu Linux          \\ \hline
GPU           & NVIDIA A100 80GB     \\ \hline
VRAM          & 80GB                  \\ \hline
Gpu Cores     & 4352                  \\ \hline
Amount        & 4 (1 for testing)     \\ \hline
\end{tabular}
\label{tab:specs}
\caption{Specs for the machines running and resting the models}
\end{table}

\begin{equation}
    TPR(TruePositiveRate/Recall) = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
    FPR(FalsePositiveRate/Recall) = \frac{FP}{FP + TN}
\end{equation}

\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}
    F1 - score = 2 x (\frac{Precision \cdot Recall}{Precision + Recall})
\end{equation}

\begin{equation}
    Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}


\begin{table}[h]
\centering
\begin{tabular}{|ll|ll|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Total Population}}} & \multicolumn{2}{c|}{Predictions}      \\ \cline{3-4} 
\multicolumn{2}{|c|}{}                                           & \multicolumn{1}{l|}{Normal} & Anomaly \\ \hline
\multicolumn{1}{|l|}{\multirow{2}{*}{Actual}}      & Normal      & \multicolumn{1}{l|}{TN}     & FP      \\ \cline{2-4} 
\multicolumn{1}{|l|}{}                             & Anomaly     & \multicolumn{1}{l|}{FN}     & TP      \\ \hline
\end{tabular}
\label{tab:confmat}
\caption{Confusion matrix}
\end{table}


\subsection{Model Training}

All models are trained using the ADAM optimizer.


\subsubsection{Training Times}
\subsubsection{Loss Values}
\subsubsection{Accuracy}

\subsection{Model Inference}

An important part of analysing our autoencoders is the choice of metrics. Whereas loss and accuracy can provide proficient details about the model training itself, other metrics are better suited for analysing the 

\subsection{Autoencoder Performance Comparison}

Table \ref{tab:autoencoder_comparison} presents a comparative analysis of the four autoencoder types: dense, convolutional, variational dense, and variational convolutional.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lcccc}
        \hline
        \textbf{Metric} & \textbf{AE} & \textbf{CNN-AE} & \textbf{VAE} & \textbf{CNN-VAE} \\
        \hline
        Reconstruction Error & 0.045 & 0.032 & 0.038 & 0.029 \\
        Latent Space Dimension & 64 & 128 & 64 & 128 \\
        Avg Training time(s) & 120 & 180 & 150 & 210 \\
        Avg Training time(s) & 120 & 180 & 150 & 210 \\
        \hline
    \end{tabular}
    \caption{Comparison of Autoencoder Performance}
    \label{tab:autoencoder_comparison}
\end{table}

\subsection{Loss Functions}

Each autoencoder type utilizes a different loss function, as outlined in Table \ref{tab:loss_functions}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{ll}
        \hline
        \textbf{Model Type} & \textbf{Loss Function} \\
        \hline
        AE & Mean Squared Error (MSE) \\
        CNN-AE & Binary Cross-Entropy \\
        \beta-VAE & MSE + KL Divergence \\
        \beta-CNN-VAE & Binary Cross-Entropy + KL Divergence \\
        \hline
    \end{tabular}
    \caption{Loss Functions for Different Autoencoder Types}
    \label{tab:loss_functions}
\end{table}

\subsection{Performance Visualization}

Figure \ref{fig:reconstruction_error} illustrates the reconstruction error for each autoencoder type across different epochs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{reconstruction_error.png}
    \caption{Reconstruction Error vs. Epochs for Different Autoencoder Types}
    \label{fig:reconstruction_error}
\end{figure}

\subsection{Latent Space Visualization}

Figure \ref{fig:latent_space} presents a t-SNE visualization of the latent space for each autoencoder type.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{latent_space_visualization.png}
    \caption{t-SNE Visualization of Latent Space for Different Autoencoder Types}
    \label{fig:latent_space}
\end{figure}

\input{chapters/resdis/models/ae}
\input{chapters/resdis/models/cae}
