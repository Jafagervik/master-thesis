\section{Future work}
\label{conc:further}

\subsection{Judas and \acrshort{das} processing}

Our program Judas is currently up to par, or even ahead, compared to existent programs for \acrshort{das} processing at \acrshort{cgf}. Even so, there is still much room for improvement and additions, including:

\begin{itemize}
    \item Optimizing the cumulative sum calculation to make it parallel, potentially using algorithms like parallel prefix sum \cite{harris2007parallel}, where \acrshort{gpu} acceleration could be introduced.
    \item Implementing a more flexible metadata handling system to accommodate datasets from other sources besides BANENOR.
    \item Extending support for multiple file formats besides \acrshort{hdf5}, such as TDMS.
    \item Developing a user-friendly interface for adjusting hardcoded values, making the program more adaptable to different experimental setups without requiring direct code modifications.
    \item Implementing more advanced denoising and signal processing techniques, potentially utilizing \acrshort{gpu} acceleration.
\end{itemize}

\subsection{TinyDAS and Autoencoder-based Anomaly Detection}

As discussed in Section \ref{disc:tinydas}, we have succeeded in many of our goals with TinyDAS. However, it is still in its early stages, and there are plenty of aspects to work on. The following is a list of further areas of improvement:

\begin{itemize}
    \item  Half-precision training has been a heavy area of attention. For now, TinyDAS supports half-precision training, inference, loss-scaling, and gradient clipping. Further improving on these to ensure gradients are within range is an important aspect of reducing runtime and memory consumption. 
    \item Implementing support for mixed precision training.
    \item Expanding support for other \acrshort{das} datasets, both from PubDAS and other public sources. This could be achieved through a more customizable format of our DataLoader class. Furthermore, implementing functionality for downloading DAS datasets directly from Globus would lower the user entry barrier.
    \item Extending the collection of models for broader comparisons between different types of models.
    \item Implementation of test datasets for supervised learning.
    \item Implementing support for distributed parallel training, enabling large-scale model training. 
    \item Expanding beyond autoencoder architectures to support other model types such as \acrshort{cnn}s and transformers.
    \item Support for recurrent layers such as LSTM, RNN, and GRU. These exist in the Tinygrad official repository but do not currently support data- or model-parallel training.
    \item Developing hybrid models that combine the temporal and spatial aspects of \acrshort{das} signals, such as CNN-LSTM architectures, or even transformer-based ones, to process and analyze \acrshort{das} data more effectively.
\end{itemize}

\subsection{Final Remarks}

We want to thank \acrfull{cgf}, and especially Robin, for providing data and computational resources. We hope both these programs can help all your current and future members further the field of \acrshort{das} research.