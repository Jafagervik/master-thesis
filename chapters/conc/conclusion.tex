\section{Conclusion}

In this thesis, four different autoencoders were trained on \acrshort{das} data on the task of anomaly detection. Additionally, a local package for loading and preprocessing \acrshort{das} data in parallel was developed for use for \acrfull{cgf}. We believe that our work, along the research provided, lay grounds for further development and improvements of both \texttt{Judas.jl} and \texttt{TinyDAS} at \acrshort{cgf}. \\ 


\textbf{G1: Evaluate the effectiveness in detecting anomalies within \acrshort{das} data}. With our autoencoder based approach, we're able to effectively detecting anomalies within \acrshort{das} data. With even bigger datasets, and more image labeling of \acrshort{das} data, we believe one can achieve even greater results. \\

\textbf{G2: Contribute to the open source community, particularly with tools and resources that are beneficial to members of \acrshort{cgf}.} Throughout our work, we've always maintained a strong stance on developing tools that are both beneficial and easy to use. We firmly believe that both of our tools can be of use, specifically for members at \acrshort{cgf}.  \\ 

\textbf{G3: Determine whether Julia is a suitable programming language for 
 high performance data processing, analysis and \acrshort{ai}, specifically aimed at \acrshort{das} data}. Overall, we find Julia satisfactory in developing novel, high performance algorithms, as well as working with data science in general, yet lacking when it comes to training \acrshort{ai} models across multiple \acrshort{gpu}s. We believe it to be an effective language of choice for members at \acrshort{cgf}.\\

\textbf{RQ 2: Can we reduce the precision of \acrshort{das} data to enable faster model training and inference without sacrificing performance?} The results suggests that we indeed can train and analyze \acrshort{das} data on smaller datatypes. Although we don't encounter many errors with the results of the detections, we encounter small errors when training using \texttt{Float16} data. \\