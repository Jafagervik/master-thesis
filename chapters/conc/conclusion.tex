\section{Conclusion}

In this thesis, four different autoencoders were trained on \acrshort{das} data on the task of anomaly detection, and . Additionally, a local package for loading and preprocessing \acrshort{das} data was developed for use for \acrfull{cgf}. \\ 


\textbf{G1: Evaluate the effectiveness in detecting anomalies within \acrshort{das} data}. lkasdfkjlsdf \\ 

\textbf{G2: Contribute to the open source community, particularly with tools and resources that are beneficial to members of \acrshort{cgf}.} Throughout our work, we've always maintained a strong stance on developing tools that are both beneficial and easy to use. We firmly believe that both of our tools can be of use, specifically for members at \acrshort{cgf}.  \\ 

\textbf{G3: Determine whether Julia is a suitable programming language for big data processing and \acrshort{ai}, specifically aimed at \acrshort{das} data}. As we noted in the discussion, we . We believe it to be an effective language of choice for members at \acrshort{cgf}. Overall, we find Julia satisfactory in developing novel, high performance algorithms, as well as working with data science in general, yet lacking when it comes to training \acrshort{ai} models across multiple \acrshort{gpu}s. \\

\textbf{RQ 2: Can we reduce the precision of \acrshort{das} data to enable faster model training and inference without sacrificing performance?} The results suggests that we indeed can train and analyze \acrshort{das} data on smaller datatypes. Although we don't encounter many errors with the results of the detections, we encounter small errors when training using \texttt{Float16} data. \\