\section{Related Work}
\label{back:relwork}


\subsection{\acrshort{das} processing techniques}

One of the file formats used for storing \acrshort{das} data is \acrshort{hdf5}. Many of the implemented libraries for \acrshort{hdf5} files now allow for parallel loading of these files. Biddiscombe et al. \cite{biddiscombe2012parallel} replaced the IO layer within a \acrshort{hdf5} library ''to allow for parallel loading between simulation and analysis''. In later years, \texttt{HDF5.jl}, the HDF5 library in Julia, allows for parallel file loading, utilizing the message-passing interface (MPI). This can potentially reduce \acrshort{das} file loading times.

An important aspect of \acrshort{das} processing revolves around frequency analysis, denoising, and other types of filtering. 2D \acrfull{fft}s within 2D linear bandpass filtering and one-dimensional adaptive filtering using \acrfull{fir} filters have all been studied and mentioned as promising processing techniques for \acrshort{das} data \cite{daspreproc}. In our preliminary studies, we conducted a performance comparison between Julia and Python for computing 2D fast fourier transforms on \acrshort{das} data using \acrshort{gpu}s. Our results demonstrated that Julia significantly outperformed Python in this specific operation \cite{projthesis}. 

Public \acrshort{das} datasets are often scarce and hard to find. PubDAS \cite{spica2023pubdas} is a public distribution of several \acrshort{das} datasets worldwide, stored in multiple file formats. The \acrfull{foresee} \cite{se-12-219-2021, zhu2023seismic, shen2021seismic, thunderddas, hone2021seismic, zhu2019penn} array is the largest dataset in the PubDAS dataset \cite{spica2023pubdas}, storing up to \qty{7}{\si{\tera\byte}} of data daily \cite{miller2023characterizing}. Researchers at Penn State University discuss ''the issue of managing large quantities of seismic data by investigating lossy wavelet compression.''. \acrshort{das} data from several years are recorded and stored in the Globus database for public use. We can use this dataset for our research mainly due to all the researchers. Many of these datasets contain scripts containing preprocessing algorithms or visualization code. These techniques are sequential, preprocessing file by file and possibly removing erroneous files. These scripts are mainly single-file Python or MatLab code. 

\subsection{Autoencoder-based Anomaly Detection}

Researchers at NTNU developed a semi-supervised LSTM Variational Autoencoder (VAE) for fault detection in maritime multi-sensor systems \cite{9514856}. Deep LSTM-based autoencoders have proven effective in detecting anomalies within multivariate time-series forecasting problems, as proven by Alaa et al. 2019 \cite{alaaDeepLstm2019}. 

An effort has been made to improve autoencoders for anomaly detection. Tan et al. \cite{tan2023improving} propose mean-shift score (MSS) to avoid ''overconfident decisions and unexpected reconstruction results'', which traditional autoencoders often struggle with, and improved outlier detection by a massive 20\%. 

Semi-supervised models are often more practical for anomaly detection. ESAD \cite{huang2021esad} proposed a new KL-divergence-based objective function for this task and proposed an encoder-decoder-encoder architecture to balance both \textit{mutual information} and \textit{entropy}. This has been applied to popular datasets such as MNIST and CIFAR-10.

Label-free autoencoder-based anomaly detection on \acrshort{das} data has been conducted as late as in 2023 by Xie et al. \cite{xie2023label}. A combination of a \acrshort{cae} model trained on normal-range \acrshort{das} data and a clustering algorithm to locate the feature center was found to beat state-of-the-art supervised networks. Another interesting aspect of this research is the emphasis on model size, creating a sufficient model with only \qty{1.34}{\si{\kilo}} parameters. This research, in particular, has led the ground for our research and the creation of a program for training and comparing several types of autoencoders. 

\subsection{Other Models and Approaches}

In general, autoencoders with linear, convolutional, or recurrent layers, clustering algorithms, and more traditional \acrshort{ml} methods have seen many use cases within \acrshort{das} research. However,  other models and architectures show promising results as well. \cite{goodfellow2014generative, goodfellow2016nips}.
%\cite{s21196627} - dnn microseismic , das
%https://www.mdpi.com/1424-8220/21/19/6627
Zhu et al. \cite{zhu2023seismic} employed ''a pre-trained PhaseNet to generate noisy labels of P/S arrivals in \acrshort{das} data'', applying the GaMMa method to refine these labels and build training datasets. They then developed a deep-learning model for earthquake detection, which yielded great results.

The CDIL-CBAM-BiLSTM model \cite{wang2024deep}, introduced as late as 2024, combines \acrshort{lstm}s with convolutional attention blocks to address the challenge of recognizing high-sampling-rate long-sequence signal data, achieving 99\% recognition accuracy. Similarly, Rahman et al. \cite{10.1115/JRC2024-124137} introduced DLSTM-SW, a deep LSTM-based sliding-window model for railway health monitoring, detecting with around 97\% accuracy.

One key aspect of \acrshort{dnn}s is the necessity of larger train datasets. Data augmentation techniques such as cropping, resizing, or color grading can increase the available datasets for more vision-based tasks \cite{shorten2019survey}. Another way to increase the total amount of train data is by leveraging \acrfulll{gan}s \cite{goodfellow2014generative, goodfellow2016nips}. After training a \acrshort{gan} model, the generator can produce data similar to already collected data. This has yielded great results on \acrshort{das} data, as demonstrated by Shiloh et al. \cite{Shiloh:19}, and can be a great way to provide more train data, which in turn can help models detect anomalies more accurately by being trained on a wider variety of data.

\acrshort{gan} have also been applied to anomaly detection in \acrshort{das} analysis. Researchers have developed LSTM-VAE-GAN models for time series anomaly detection \cite{s20133738}, as well as modified LSTM-GANs incorporating attention mechanisms. 

Furthermore, \acrshort{cnn} models with attention mechanisms have shown promise in denoising \acrshort{das} signals \cite{eage:/content/journals/10.1111/1365-2478.13355}. In pursuit of real-time applications, a ResNet \cite{koonce2021resnet} with Convolutional Block Attention Module (CBAM) achieved inference times as low as \textit{\qty{3.3}{\milli\second}} per sample while maintaining high accuracy for multi-scenario scenes as proposed by Liu et al. \cite{photonics9100677}. Their focus on inference speed and accuracy prompted our interest in comparing half-precision accuracies with single-precision for possible enhanced efficiency in offline and online scenarios.

Interested in multi-variate spatio-temporal data, Karadayi et. al \cite{hybriddlspatio} introduced a hybrid model 
combining a \acrshort{cnn} with \acrshort{lstm} to detect anomalies during Hurricane Katrina, which resulted in more than 10\% better accuracy than other unsupervised anomaly detection methods. Furthermore, \acrshort{cnn}s, combined with \acrshort{lstm}s \textit{and} \acrshort{fcnn}s have been applied to earthquake detection using \acrshort{das}. Their standalone \acrshort{cnn} approach reached 96.94\% accuracy, beating their combined version by 3\%. Two interesting aspects of their research directly prompted some of our methodology, mainly:
\begin{enumerate}
    \item The spatial characteristics of \acrshort{das} signals alone being of high importance, yielding better results than their combined version
    \item Their strategy of not depending on too large datasets
\end{enumerate}
\subsection{Research Gaps and Potential}
Despite the significant progress in processing and anomaly detection of \acrshort{das} data, several areas require further exploration. Many current studies do not fully address the constraints of available computational power or the impact of half-precision inference in anomaly detection of large-scale dense data. Furthermore, although some of these highlight different scores and measurements besides accuracy, future work could highlight even more computational benchmarks, such as model size and inference speed. Finally, a fully open-source comprehensive benchmark between different approaches over various datasets, such as the ones in PubDAS, can help forward research on efficient \acrshort{das} handling and \acrshort{das} data.





