\section{Unsupervised Learning}

The major bottleneck of all kinds of machine learning tecniques is data. The more diverse and varied a .

When it comes to \acrshort{ai} and \acrshort{ml} we usually differentiate between tree major types, those being supervised, unsupervised and semi-supervised learning (or self-supervised learning). They differ in the roles that can occur.


\subsection{Linear Layers}

Fully Connected Layer, Dense Layer or Linear Layers  are types of an operation used in several deep learning models. This layer take an input vector and maps it to an output of learnable parameters. The resulting parameters are thus a weight matrix $W$ and a bias vector $b$. It is defined as follows:

\begin{align}
    Y &= XW + b
\end{align}

where:
\begin{align*}
    X & \text{ - input vector [$n x m$] where $m$ is the amount of input features and $n$ is the batch size} \\
    W & \text{ - weight matrix [$m x p$] where $p$ is the amount of output features} \\
    b & \text{ - bias vector of size $p$} \\
    Y & \text{ - output vector [$n x p$]} \\
\end{align*}

Furthermore, an activation function such as \gls{relu} \ref{eq:relu} is applied to the output to achieve nonlinearity.
Lets denote this function as $f$, we now get the following equation.

\begin{equation}
    Z = f(Y) = f(XW+b)
\end{equation}

Linear layers have several advantages, such as computational efficiency, flexibility as well as intrerprebility, where the weight and bias vectors can be interpreted as learned parameters. They also serve as building blocks for other components, such as \acrshort{rnn}s, \acrshort{lstm}s or even attention blocks. \\ 

They are however prone to overfitting and linearity, due to their inherent linearity. They can become quite limited when trying to extract more complex features, thus reducing some of the discriminative power.