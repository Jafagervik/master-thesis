\subsection{LSTM - Long Short Term Memory}

One of two more common solutions to avoid the pitfalls \acrshort{rnn}s give us, is to use \acrshort{lstm} cells. 

 As we've mentioned, regular \acrshort{rnn}s struggle with long term memory, something the \acrshort{lstm} cells solves. \\ 


An \acrshort{lstm} cell is built up by the following components: an input gate $i$, a forget gate $f$, a candidate state $g$, an output gate $g$ .
These cells traditionally make use of the sigmoid non-linearity function $\sigma()$

\begin{figure}[h]
    \centering
    \includegraphics[scale=.4]{figures/lstmcell.png}
    \caption{Example of an LSTM cell}
    \label{fig:lstmcell}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=.4]{figures/grucell.png}
    \caption{Example of an GRU cell}
    \label{fig:lstmcell}
\end{figure}

Compared to the equation for forward pass 

\begin{equation} \label{eq:cnn}
    Y_t = W_xx_t + b
\end{equation}

the \acrshort{lstm} keeps the previous state in mind, thus giving us the equation


\begin{equation} \label{eq:lstm}
    Y_t = W_hh_{t-1} + W_xx_t + b
\end{equation}

Adding batch normalization to this equation and we end up with the following equation \cite{cooijmans2017recurrent}: 

\begin{equation} \label{eq:bnlstm}

c_t = \sigma(\Tilde{\textbf{f}_t} \cdot c_{t-1} + \sigma(\textbf{\Tilde{i}}_t) \cdot tanh(\Tilde{g}_t)
h_t = \sigma(\Tilde{o}_t) \cdot tanh(BN(\textbf{c_t}; \gamma_c, \beta_c))
    
\end{equation}

By applying batch normalization to \acrshort{lstm}s, not only did the models converge faster, the performance was up to par with the unnormalized \acrshort{lstm} \cite{cooijmans2017recurrent}.

Whereas \acrshort{rnn}s shines at \acrshort{nlp}, speech recognition, and media processing, \acrshort{lstm}s is vastly better for time series forecasting due to the memory gates. This makes \acrshort{lstm}s a suitable option for us when working with anomaly detection on sensor data, which in its essence is nothing more than more complex time series data across multiple columns or channels. 

The alternative to using \acrshort{lstm} nodes for our network would be \acrfull{gru}s. Gated recurrent units are ... Simple recurrent units are \cite{lei2018simple}.


\subsubsection{Loss functions on LSMTS}

In typical LSTM 

\acrfull{mae} is the most common algorithm when it 

\subsubsection{KL Divergence}

Another high relevant loss function to look at is the KL divergence (Kullback-Leibler divergence)  \cite{shlens2014notes}. It is used to measure distance or similarity between two probability distributions. Given the prior distribution $P$ and the posterior $Q$, it is described as follows:

\begin{equation}
    \text{KL}(P||Q) = \int_{-\infty}^{\infty} P(x) \log \left( \frac{P(x)}{Q(x)} \right) \, dx
\end{equation}

It is worth nothing that the KL divergence is non-negative $KL \geq 0$, thus meaning it is not a distance metric.
Regarding variational autoencoders, its objective is to measure discrepancy between the prior and the posterior over latent variables. It is 

\subsubsection{ELBO Loss}

ELBO (Evidenve Lower BOund) loss is 

\subsubsection{Activation functions}

To be able to develop complex representations and functions based on inputs, nonlinearity is a necessity. To be able to achieve this, nonlinear activation functions are introduced. Some of the most commonly used activation functions in deep learning are tanh, sigmoid and \acrshort{relu} which are displayed in \ref{fig:actfuncs}.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{groupplot}[
    group style={
        group size=3 by 1,
        horizontal sep=2cm
    },
    width=0.3\textwidth,
    height=0.3\textwidth,
    xlabel=$x$,
    ylabel=$y$,
    xmin=-5, xmax=5,
    ymin=-1.2, ymax=1.2,
    xtick={-4,-2,0,2,4},
    ytick={-1,-0.5,0,0.5,1},
    legend pos=north west,
    legend style={font=\small},
    title style={font=\small},
]

% Sigmoid
\nextgroupplot[title=Sigmoid]
\addplot[blue, domain=-5:5, samples=100] {1 / (1 + exp(-x))};

% Tanh
\nextgroupplot[title=Tanh]
\addplot[red, domain=-5:5, samples=100] {tanh(x)};

% ReLU
\nextgroupplot[title=ReLU]
\addplot[green, domain=-5:0, samples=2] {0};
\addplot[green, domain=0:5, samples=2] {x};

\end{groupplot}
\end{tikzpicture}
\caption{Comparison of Sigmoid, Tanh, and ReLU activation functions}
\label{fig:actfuncs}
\end{figure}

\paragraph{Tanh} also known as hyperbolic function is ...

\begin{equation}
    tanh(z) = \dfrac{sinh(z)}{cosh(z)} = \dfrac{e^{2z} - 1}{e^{2z} + 1}
\label{eq:tanh}
\end{equation}

\paragraph{ReLU} is commonly used in many different types of neural networks. Compared to the other alternatives, it introduce low computational overhead, yet still achives non linearity. Many variations of relu, such as leaky relu, gelu and relu6, try to improve on relu, yet the classic relu still reigns as the top alternative for many.

\begin{equation}
    ReLU(z) = max(0, z)
\label{eq:relu}
\end{equation}

\paragraph{Sigmoid} is popular due to 

\begin{equation}
    \Phi(z) = \dfrac{1}{1 + e^{-z}}
\label{eq:sigmoid}
\end{equation}

\subsubsection{Optimizers}

Choosing the correct optimizer for the neural network of choice can drastically change the results and training time. During the years, several optimizers have been shown to produce great results. Some of these are \acrfull{sgd}, RMSprop and \acrfull{adagrad}. \acrfull{adam}, first introduced in 2017, \cite{kingma2017adam}, is a continuation and improvement upon the two latter ones. Known for its computational efficiency, low memory requirement and straightforwardness to implement, it's now the \textit{de-facto standard} of optimizers. It is defined as follows:


\begin{align}
    m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
    v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
    \hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
    \hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
    \theta_{t+1} &= \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}

where:
\begin{align*}
    \theta_t & \text{ - parameters at time step } t \\
    g_t & \text{ - gradient of the loss function at time step } t \\
    \alpha & \text{ - learning rate} \\
    \beta_1, \beta_2 & \text{ - exponential decay rates for the moment estimates} \\
    m_t, v_t & \text{ - first and second moment estimates} \\
    \hat{m}_t, \hat{v}_t & \text{ - bias-corrected first and second moment estimates} \\
    \epsilon & \text{ - small constant to prevent division by zero}
\end{align*}

The original paper sets the following parameters for ADAM: $\beta_1 = 0.99, \beta_2 = 0.9, \mu=10^{-8}$ and $\alpha = 0.001$ \cite{kingma2017adam}.